{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c702a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:06:21.279568Z",
     "start_time": "2021-07-02T07:06:21.265299Z"
    }
   },
   "source": [
    "<span style=\"color:red\"><b>  생각해보기  </b></span>  <br>\n",
    "* data에 outlier에 대해서도 해야하니 quantile regression 이 적합?? <br>\n",
    "    -> probability 처럼 나타낼 수도 있음 <br>\n",
    "    \n",
    "    \n",
    "* 바람 쌘 날이 잘 반영되지 않음 <br>\n",
    "    -> 어떤 새로운 변수를 만들어야 하나??<br>\n",
    "    \n",
    "    \n",
    "* TMA Capacity 정의 <br>\n",
    "    TMA와 Aiport의 Capacity에 차이라고 한다면 holding으로 인한 것 뿐인 것 같음 <br>\n",
    "    -> 이 부분만 airport capacity에 포함을 시키면 되지않을까? <br>\n",
    "    \n",
    "  \n",
    "* 이 ML이 prediction이라고 주는 값은,  Demand가 ~고, 날씨가 ~일 떄 몇대가 들어올 것 같다는 값을 줌 <br>\n",
    "    -> 이 항공기 숫자를 robust하게 예측하게 하고(확률적으로) reservior 개념을 적용해서 scheduling problem으로 발전(holding은 어떻게 넣을래?)  <br>\n",
    "    \n",
    "    \n",
    "* \n",
    "<span style=\"color:red\"> *** </span>\n",
    "<b> reservoir를 하려면 AAR, ADR을 STAR, SID에 들어오는 수로 해야겠는데??? (특히 Arrival) </b> <br>\n",
    "    그래야지 TMA용량 정의를 holding하는 항공기 수를 포함하여 적용가능<br>\n",
    "    departure는 거의 공항과 같게 될 것 같음(departure는 홀딩을 거의 공항에서 하므로) , 아니면 departure로 hand off되는 고도에 도착하는 걸로  <br><br>\n",
    "    \n",
    "* Max capacity에 영향을 주는 것 <br>\n",
    "관제사 워크로드 <br>\n",
    "기상 <br>\n",
    "항공기 성능(어떤 어프로치 가능한지 등) <br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "***    \n",
    "<br>\n",
    "예를들어, reservoir에서 15분까지 airborne holding까지 허용한다고 하자 <br>\n",
    "지금 방법으로하면 Demand만 한대씩 늘려가면서, 몇 대가 들어올 지 예측해서 값을 주는 거지 <br>\n",
    "(즉, A대가 그떄 도착하도록 이렇게저렇게 GDP를 조정해서 보낸다 하면, 그 중 몇대가 B대가 그시간대에 올지 예측해서 값으로 줌) <br>\n",
    "-> 이렇게 한대씩 늘렸을 때 예측값으로 준 rate과 불확실성을 대변하는 무엇(별표*)을 가지고 최대 15분 holding이라는 기준을 맞추도록 stochastic scheduling을 해서 최종적으로 적절하게 보내도 되는 A값을 찾음 (여러 곳에서 오는 신호를 순서대로 처리하라고 보내는 것이라고 할 수도 있음)  <br>\n",
    "-> A값을 그 상황에서의 Capacity 로 정의 <br>\n",
    "<br>\n",
    "별표(*) <br>\n",
    "몇 대가 올지 예측한 거를 확률로?<br>\n",
    "근데 어디서 올지, 언제 올지를 모른단 말이지 -> 이것도 확률로??<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48b304",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a132f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:00:54.273225Z",
     "start_time": "2021-07-09T09:00:49.749567Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna.integration.lightgbm as lgb          ####\n",
    "from lightgbm import LGBMRegressor, plot_metric    #### 여기서 lgb로 안가져옴\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, max_error, mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd179e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:00:54.321255Z",
     "start_time": "2021-07-09T09:00:54.306155Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# options\n",
    "pd.set_option('max_columns',100)\n",
    "plt.style.use('fivethirtyeight')\n",
    "warnings.simplefilter('ignore')\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aff956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:00:54.369358Z",
     "start_time": "2021-07-09T09:00:54.355362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data dirctory\n",
    "data_dir = Path('../data/')\n",
    "data_file = data_dir / 'Data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3108e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:00:54.607282Z",
     "start_time": "2021-07-09T09:00:54.400938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "# 0:AAR / 1:EAD / 2:ADR / 3:EDD는 고정  , 나머지는 순서 상관 없음\n",
    "Data = pd.read_csv(data_file, index_col=0)\n",
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf97b8",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e72fe6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666baaf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T07:45:04.447686Z",
     "start_time": "2021-07-08T07:44:58.287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# decision tree류의 알고리즘은 Scaling(standardization, normalization)이 큰 의미 X\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[num_cols])\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, np.log1p(df[target_col]))\n",
    "df[pred_col] = np.expm1(lr.predict(X))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df[num_cols])\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, np.log1p(df[target_col]))\n",
    "df[pred_col] = np.expm1(lr.predict(X))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7e960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T07:31:06.856765Z",
     "start_time": "2021-07-09T07:31:06.846631Z"
    }
   },
   "outputs": [],
   "source": [
    "#Binning\n",
    "\n",
    "#어떤 feature를 n개의 그룹으로 나누고, 그것을 새로운 categorical data로 넣는 것\n",
    "\n",
    "\"\"\"\n",
    "df['time_bin'] = pd.qcut(df['time'], 4, labels=False)    # 같은 개수가 들어가도록 4그룹으로 자르기\n",
    "sns.pairplot(data=df, vars=['time', 'time_bin'], size=4, plot_kws={'alpha': .5})\n",
    "\n",
    "\n",
    "X = pd.concat([df[num_cols], pd.get_dummies(pd.qcut(df['time'], 4, labels=False))], axis=1)    \n",
    "# get_dummies 는 one-hot encoding해주는 것(decision tree 계열을 안 하는게 보통 더 좋은 결과를 냄)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, np.log1p(df[target_col]))\n",
    "df[pred_col] = np.expm1(lr.predict(X))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a6adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:00:57.229087Z",
     "start_time": "2021-07-09T09:00:57.208900Z"
    }
   },
   "outputs": [],
   "source": [
    "Data['WS_over20'] = 0\n",
    "Data['WS_over20'][Data['WSPD']>20] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb31edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T07:45:04.449686Z",
     "start_time": "2021-07-08T07:44:58.294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "# 연속적인 몇개의 feature들을 조합해서 새로운 feature를 만드는 것 (overfitting 위험 O)\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)        # 2차(x^2, x1 * x2 등) 까지만 만들겠다\n",
    "X = poly.fit_transform(df[num_cols])\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, np.log1p(df[target_col]))\n",
    "df[pred_col] = np.expm1(lr.predict(X))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ab841",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T07:45:04.450684Z",
     "start_time": "2021-07-08T07:44:58.296Z"
    }
   },
   "source": [
    "Prepare data\n",
    "\n",
    "1. Checking for NaN values and removing constant features in the training data\n",
    "2. Removing duplicated columns\n",
    "3. Drop Sparse Data\n",
    "\n",
    "Add Features<br>\n",
    "1. Sumzeros and Sumvalues \n",
    "2. Other Aggregates\n",
    "3. K-Means \n",
    "4. PCA : Principal component analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea184e1",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a6792",
   "metadata": {},
   "source": [
    "# Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b62415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:00.508845Z",
     "start_time": "2021-07-09T09:01:00.367604Z"
    }
   },
   "outputs": [],
   "source": [
    "# 필요없는 것을 버리기\n",
    "Data_temp = Data.drop('TMP', axis=1)\n",
    "Data_temp = Data_temp.drop('TD', axis=1)\n",
    "Data_temp = Data_temp.drop('HM', axis=1)\n",
    "Data_temp = Data_temp.drop('PS', axis=1)\n",
    "Data_temp = Data_temp.drop('PA', axis=1)\n",
    "\n",
    "#고층바람 너무 높은 고도는 뺴자 \n",
    "Data_temp = Data_temp.drop('WD_400', axis=1)\n",
    "Data_temp = Data_temp.drop('WD_500', axis=1)\n",
    "Data_temp = Data_temp.drop('WD_700', axis=1)\n",
    "Data_temp = Data_temp.drop('WS_400', axis=1)\n",
    "Data_temp = Data_temp.drop('WS_500', axis=1)\n",
    "Data_temp = Data_temp.drop('WS_700', axis=1)\n",
    "\n",
    "# drop TAF\n",
    "for i in range(6,30,6):\n",
    "    Data_temp = Data_temp.drop(f'WDIR_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'WSPD_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'WG_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'VIS_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'WC_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'CLA_1LYR_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'BASE_1LYR_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'CLA_2LYR_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'BASE_2LYR_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'CLA_3LYR_t{i}', axis=1)\n",
    "    Data_temp = Data_temp.drop(f'BASE_3LYR_t{i}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557af25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:01.084869Z",
     "start_time": "2021-07-09T09:01:00.994123Z"
    }
   },
   "outputs": [],
   "source": [
    "# 각 시간에 맞는 TAF로 나누기\n",
    "taf6 = [12,18,24]\n",
    "taf12 = [6,18,24]\n",
    "taf18 = [6,12,24]\n",
    "taf24 = [6,12,18]\n",
    "    \n",
    "# 각 시간에 맞는 taf 넣기\n",
    "data_taf = {}\n",
    "for i in range(6,30,6):\n",
    "    data_taf[f'Data_{i}'] = Data_temp    \n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'WDIR_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'WSPD_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'WG_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'VIS_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'WC_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'CLA_1LYR_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'BASE_1LYR_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'CLA_2LYR_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'BASE_2LYR_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'CLA_3LYR_t{i}'])\n",
    "    data_taf[f'Data_{i}'] = data_taf[f'Data_{i}'].join(Data[f'BASE_3LYR_t{i}'])\n",
    "    \n",
    "Data_6 = data_taf['Data_6']\n",
    "Data_12 = data_taf['Data_12']\n",
    "Data_18 = data_taf['Data_18']\n",
    "Data_24 = data_taf['Data_24']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50041d29",
   "metadata": {},
   "source": [
    "Validation <br><br>\n",
    "\n",
    "sklearn.model_selection\n",
    "* train_test_split\n",
    "* KFold\n",
    "* StratifiedKFold\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "lightGBM <br>\n",
    "* lgb.cv    # 쓰려면 lightgbm을 lgb로 import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69db9d5",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3c018",
   "metadata": {},
   "source": [
    "# LightGBM fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f29bb",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b62e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:04:14.877445Z",
     "start_time": "2021-07-09T09:04:14.805888Z"
    }
   },
   "outputs": [],
   "source": [
    "# 예측할 시간에 맞는 Data로 넣기\n",
    "# 0-6 : Data_6 / 6-12 : Data_12 / 12-18 : Data_18 / 18-24 : Data_24\n",
    "Data_raw = Data_6\n",
    "Data_m = Data_6\n",
    "Data_m = Data_m.drop('AAR', axis=1)\n",
    "Data_m = Data_m.drop('ADR', axis=1)\n",
    "\n",
    "\n",
    "# Arrival\n",
    "y_a = Data_raw.AAR.to_numpy()\n",
    "X_a = Data_m.to_numpy()\n",
    "#X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size = 0.1, random_state = seed)\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size = 0.1, random_state = seed)\n",
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_train_a, y_train_a, test_size=0.11, random_state = 13) \n",
    "\n",
    "\n",
    "# Departure\n",
    "y_d = Data_raw.ADR.to_numpy()\n",
    "X_d = Data_m.to_numpy()\n",
    "#X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size = 0.1, random_state = seed)\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size = 0.1, random_state = seed)\n",
    "X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X_train_d, y_train_d, test_size=0.11, random_state = 13) \n",
    "\n",
    "# val은 hyperparameter 검증에 사용\n",
    "# 0.11 x 0.9 = 0.099\n",
    "\n",
    "print(Data_m.shape)\n",
    "Data_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da30602",
   "metadata": {},
   "source": [
    "***\n",
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e435c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:06.698687Z",
     "start_time": "2021-07-09T09:01:06.686825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "params = {'boosting_type' : 'gbdt',            # 'dart' 는 계산시간 길어짐, early stopping X / 'rf’ : Random Forest\n",
    "          'metric': 'mse',\n",
    "          'num_leaves' : 127,                  ## Maximum tree leaves for base learners (31)\n",
    "          'max_depth' : - 1,                   # Maximum tree depth for base learners, <=0 means no limit (-1)\n",
    "          'learning_rate' : 0.001,             ## Boosting learning rate (0.1)\n",
    "          'n_estimators' : 10000000,           # Number of boosted trees to fit (100) -> fit에서 early stopping으로 제한해서 크게 설정함\n",
    "          'subsample_for_bin' : 200000,        # Number of samples for constructing bins (200000)\n",
    "          'objective' : 'regression',          # learning task and the corresponding learning objective (None)\n",
    "          'class_weight' : None,               # * Use this parameter only for multi-class classification task\n",
    "          'min_split_gain' : 0.0,              # Minimum loss reduction required to make a further partition on a leaf node of the tree (0)\n",
    "          'min_child_weight' : 0.001,          # Minimum sum of instance weight (hessian) needed in a child (leaf) (0.001)\n",
    "          'min_child_samples' : 1,             # Minimum number of data needed in a child (leaf) (20) - 마지막노드(리프)에 최소 몇가지 샘플이 있어야 하는지 \n",
    "          'subsample' : 0.8,                   ## Subsample ratio of the training instance (1.0) - 개별 트리를 학습시키는데 몇 %의 데이터를 사용할 것 인지, row sampling\n",
    "          'subsample_freq' : 1,                # Frequency of subsample, <=0 means no enable (0) - 몇개의 트리마다 subsampling을 할 것인지\n",
    "          'colsample_bytree' : 0.8,            ## Subsample ratio of columns when constructing each tree (1.0) - 몇 %의 column을 sampling 할 것인지\n",
    "          'reg_alpha' : 0.0,                   # L1 regularization term on weights (0)\n",
    "          'reg_lambda' : 0.0,                  # L2 regularization term on weights (0)\n",
    "          'random_state' : seed,               # Random number seed (None)\n",
    "          'n_jobs' : - 1,                      # Number of parallel threads (-1) - 몇 개의 병렬작업을 할 것인지 (-1 = 모든 가능한 것 전부)\n",
    "          'silent' : True,                     # Whether to print messages while running boosting (True)\n",
    "          'importance_type' : 'split'}         # ‘split’: result contains numbers of times the feature is used in a model\n",
    "                                               # ‘gain’ : result contains total gains of splits which use the feature\n",
    "\n",
    "# 최적 hyperparameter 찾기!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706352d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:08.123321Z",
     "start_time": "2021-07-09T09:01:08.109663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters - arrival\n",
    "params_a = {'boosting_type' : 'gbdt',                   # 'dart' 는 계산시간 길어짐, early stopping X / 'rf’ : Random Forest\n",
    "            'metric': 'mse',\n",
    "            'num_leaves' : 127,                          ## Maximum tree leaves for base learners (31)\n",
    "            'max_depth' : - 1,                          # Maximum tree depth for base learners, <=0 means no limit (-1)\n",
    "            'learning_rate' : 0.001,                    ## Boosting learning rate (0.1)\n",
    "            'n_estimators' : 10000000,                  # Number of boosted trees to fit (100) -> fit에서 early stopping으로 제한해서 크게 설정함\n",
    "            'subsample_for_bin' : 200000,               # Number of samples for constructing bins (200000)\n",
    "            'objective' : 'regression',                 # learning task and the corresponding learning objective (None)\n",
    "            'class_weight' : None,                      # * Use this parameter only for multi-class classification task\n",
    "            'min_split_gain' : 0.0,                     # Minimum loss reduction required to make a further partition on a leaf node of the tree (0)\n",
    "            'min_child_weight' : 0.001,                 # Minimum sum of instance weight (hessian) needed in a child (leaf) (0.001)\n",
    "            'min_child_samples' : 1,                    # Minimum number of data needed in a child (leaf) (20) - 마지막노드(리프)에 최소 몇가지 샘플이 있어야 하는지 \n",
    "            'feature_pre_filter': False,\n",
    "            'subsample' : 0.8,           ## Subsample ratio of the training instance (1.0) - 개별 트리를 학습시키는데 몇 %의 데이터를 사용할 것 인지, row sampling\n",
    "            'subsample_freq' : 1,   #3                  # Frequency of subsample, <=0 means no enable (0) - 몇개의 트리마다 subsampling을 할 것인지\n",
    "            'colsample_bytree' : 0.6839999999999999,    ## Subsample ratio of columns when constructing each tree (1.0) - 몇 %의 column을 sampling 할 것인지\n",
    "            'reg_alpha' : 1.5913347234314888e-05,       # L1 regularization term on weights (0)\n",
    "            'reg_lambda' : 0.8083105161094011,           # L2 regularization term on weights (0)\n",
    "            'random_state' : seed,                      # Random number seed (None)\n",
    "            'n_jobs' : - 1,                             # Number of parallel threads (-1) - 몇 개의 병렬작업을 할 것인지 (-1 = 모든 가능한 것 전부)\n",
    "            'silent' : True,                            # Whether to print messages while running boosting (True)\n",
    "            'importance_type' : 'split'}                # ‘split’: result contains numbers of times the feature is used in a model\n",
    "                                                        # ‘gain’ : result contains total gains of splits which use the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf8758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:07.429294Z",
     "start_time": "2021-07-09T09:01:07.415651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optuna 간단 버전 - arrival\n",
    "\n",
    "\"\"\"\n",
    "y = Data_raw.AAR.to_numpy()\n",
    "X = Data_m.to_numpy()\n",
    "\n",
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X, y, test_size=.2, random_state=seed)\n",
    "\n",
    "dtrain_a = lgb.Dataset(X_train_a, label = y_train_a)\n",
    "dval_a = lgb.Dataset(X_val_a, label = y_val_a)\n",
    "\n",
    "model = lgb.train(params, dtrain_a,\n",
    "                  valid_sets=[dtrain_a, dval_a], \n",
    "                  verbose_eval=100,\n",
    "                  early_stopping_rounds=10)\n",
    "\n",
    "prediction = model.predict(X_val_a, num_iteration=model.best_iteration)\n",
    "\n",
    "accuracy = r2_score(y_val_a, prediction)        # classification이면 앞에 argmax( , axis = 1)로 하면 될 듯\n",
    "\n",
    "\n",
    "\n",
    "params = model.params\n",
    "print(\"Best params:\", params)\n",
    "print(\"  Accuracy = {}\".format(accuracy))\n",
    "print(\"  Params: \")\n",
    "for key, value in params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "params_a = params\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac52ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:09.490312Z",
     "start_time": "2021-07-09T09:01:09.477140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters - departure\n",
    "params_d = {'boosting_type' : 'gbdt',                 # 'dart' 는 계산시간 길어짐, early stopping X / 'rf’ : Random Forest\n",
    "            'metric': 'mse',\n",
    "            'num_leaves' : 127,                        ## Maximum tree leaves for base learners (31)\n",
    "            'max_depth' : - 1,                        # Maximum tree depth for base learners, <=0 means no limit (-1)\n",
    "            'learning_rate' : 0.001,                  ## Boosting learning rate (0.1)\n",
    "            'n_estimators' : 10000000,                # Number of boosted trees to fit (100) -> fit에서 early stopping으로 제한해서 크게 설정함\n",
    "            'subsample_for_bin' : 200000,             # Number of samples for constructing bins (200000)\n",
    "            'objective' : 'regression',               # learning task and the corresponding learning objective (None)\n",
    "            'class_weight' : None,                    # * Use this parameter only for multi-class classification task\n",
    "            'min_split_gain' : 0.0,                   # Minimum loss reduction required to make a further partition on a leaf node of the tree (0)\n",
    "            'min_child_weight' : 0.001,               # Minimum sum of instance weight (hessian) needed in a child (leaf) (0.001)\n",
    "            'min_child_samples' : 1,                  # Minimum number of data needed in a child (leaf) (20) - 마지막노드(리프)에 최소 몇가지 샘플이 있어야 하는지 \n",
    "            'feature_pre_filter': False,\n",
    "            'subsample' : 0.9220698151647799,         ## Subsample ratio of the training instance (1.0) - 개별 트리를 학습시키는데 몇 %의 데이터를 사용할 것 인지, row sampling\n",
    "            'subsample_freq' : 1,    #3               # Frequency of subsample, <=0 means no enable (0) - 몇개의 트리마다 subsampling을 할 것인지\n",
    "            'colsample_bytree' : 0.8480000000000001,  ## Subsample ratio of columns when constructing each tree (1.0) - 몇 %의 column을 sampling 할 것인지\n",
    "            'reg_alpha' : 1.2560334090582146e-07,     # L1 regularization term on weights (0)\n",
    "            'reg_lambda' : 0.003315287591858434,      # L2 regularization term on weights (0)\n",
    "            'random_state' : seed,                    # Random number seed (None)\n",
    "            'n_jobs' : - 1,                           # Number of parallel threads (-1) - 몇 개의 병렬작업을 할 것인지 (-1 = 모든 가능한 것 전부)\n",
    "            'silent' : True,                          # Whether to print messages while running boosting (True)\n",
    "            'importance_type' : 'split'}              # ‘split’: result contains numbers of times the feature is used in a model\n",
    "                                                      # ‘gain’ : result contains total gains of splits which use the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4d4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:08.813187Z",
     "start_time": "2021-07-09T09:01:08.799861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optuna 간단 버전 - departure\n",
    "\n",
    "\"\"\"\n",
    "y = Data_raw.ADR.to_numpy()\n",
    "X = Data_m.to_numpy()\n",
    "\n",
    "X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X, y, test_size=.2, random_state=seed)\n",
    "\n",
    "dtrain_d = lgb.Dataset(X_train_d, label = y_train_d)\n",
    "dval_d = lgb.Dataset(X_val_d, label = y_val_d)\n",
    "\n",
    "model = lgb.train(params, dtrain_d,\n",
    "                  valid_sets=[dtrain_d, dval_d], \n",
    "                  verbose_eval=100,\n",
    "                  early_stopping_rounds=10)\n",
    "\n",
    "prediction = model.predict(X_val_d, num_iteration=model.best_iteration)\n",
    "\n",
    "accuracy = r2_score(y_val_d, prediction)         # classification이면 앞에 argmax( , axis = 1)로 하면 될 듯\n",
    "\n",
    "params = model.params\n",
    "print(\"Best params:\", params)\n",
    "print(\"  Accuracy = {}\".format(accuracy))\n",
    "print(\"  Params: \")\n",
    "for key, value in params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "params_d = params\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4778b3",
   "metadata": {},
   "source": [
    "***\n",
    "## Arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11e024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:01:30.900133Z",
     "start_time": "2021-07-09T09:01:10.167649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "reg_arrival = LGBMRegressor(**params_a)\n",
    "\n",
    "reg_arrival.fit(X_train_a, y_train_a,\n",
    "                sample_weight = None,                   # Weights of training data\n",
    "                init_score = None,                      # Weights of training data\n",
    "                eval_set = [(X_val_a, y_val_a)],        # pairs to use as validation sets\n",
    "                eval_sample_weight = None,              # Weights of eval data\n",
    "                eval_init_score = None,                 # Init score of eval data.   \n",
    "                eval_metric = 'l2',                     # Default: ‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier\n",
    "                early_stopping_rounds = 10 )            # loss fuction이 n번 이상 좋아지지 않으면 멈춰라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e4655",
   "metadata": {},
   "source": [
    "***\n",
    "##  Departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395c91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:02:04.689780Z",
     "start_time": "2021-07-09T09:01:31.699695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "reg_departure = LGBMRegressor(**params_d)\n",
    "\n",
    "reg_departure.fit(X_train_d, y_train_d,\n",
    "                  eval_set=[(X_val_d, y_val_d)],\n",
    "                  eval_metric='l2',\n",
    "                  early_stopping_rounds = 10)     # loss fuction이 n번 이상 좋아지지 않으면 멈춰라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe582042",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2be771",
   "metadata": {},
   "source": [
    "for loop안에서 predict하고 평균을 내야 함 <br>\n",
    "-> demand를 하나씩 증가시키리면, 바로 CV를 사용하기는 어려울 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3c1a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T07:45:04.461686Z",
     "start_time": "2021-07-08T07:44:58.334Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "n_fold = 10\n",
    "\n",
    "cv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "p_val_a = np.zeros(X_a.shape[0])\n",
    "predict_a = np.zeros(X_a.shape[0])\n",
    "p_val_d = np.zeros(X_d.shape[0])\n",
    "predict_d = np.zeros(X_d.shape[0])\n",
    "\n",
    "############### 위에서 X_val_a등으로 바꾼 거 없애고 -> 아래 X_a, y_a, X_d, y_d를 X_train_a 등으로 바꾸기 #######################\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(X_a, y_a), 1):        # 몇번째인지 보기 위해 enumerate 사용\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print(f'Training model for Cross-Validation #{i}')\n",
    "    reg_arrival = LGBMRegressor(**params)    #############params_a\n",
    "    reg_arrival.fit(X_a[i_trn], y_a[i_trn],\n",
    "                sample_weight = None,                   # Weights of training data\n",
    "                init_score = None,                      # Weights of training data\n",
    "                eval_set = [(X_a[i_val], y_a[i_val])],  # pairs to use as validation sets\n",
    "                eval_sample_weight = None,              # Weights of eval data\n",
    "                eval_init_score = None,                 # Init score of eval data.   \n",
    "                eval_metric = 'l2',                     # Default: ‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier, ‘ndcg’ for LGBMRanker\n",
    "                early_stopping_rounds = 10 )            # loss fuction이 n번 이상 좋아지지 않으면 멈춰라\n",
    "    \n",
    "    p_val_a[i_val] = reg_arrival.predict(X_a[i_val])\n",
    "    predict_a += reg_arrival.predict(X_a) / n_fold      ##### 이 자리에 원래는 test data가 들어가면 됨\n",
    "                                                        # test data의 prediction은 CV에서 각 dataset이 예측한 값들의 평균이므로\n",
    "    \n",
    "for i, (i_trn, i_val) in enumerate(cv.split(X_d, y_d), 1):        # 몇번째인지 보기 위해 enumerate 사용\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print(f'Training model for Cross-Validation #{i}')\n",
    "    reg_departure = LGBMRegressor(**params)    #############params_d\n",
    "    reg_departure.fit(X_d[i_trn], y_d[i_trn],\n",
    "                      sample_weight = None,                   # Weights of training data\n",
    "                      init_score = None,                      # Weights of training data\n",
    "                      eval_set = [(X_d[i_val], y_d[i_val])],  # pairs to use as validation sets\n",
    "                      eval_sample_weight = None,              # Weights of eval data\n",
    "                      eval_init_score = None,                 # Init score of eval data.   \n",
    "                      eval_metric = 'l2',                     # Default: ‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier, ‘ndcg’ for LGBMRanker\n",
    "                      early_stopping_rounds = 10 )            # loss fuction이 n번 이상 좋아지지 않으면 멈춰라\n",
    "    \n",
    "    p_val_d[i_val] = reg_departure.predict(X_d[i_val])\n",
    "    predict_d += reg_departure.predict(X_d) / n_fold          ##### 이 자리에 원래는 test data가 들어가면 됨\n",
    "                                                              # test data의 prediction은 CV에서 각 dataset이 예측한 값들의 평균이므로\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b4cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T07:45:04.462687Z",
     "start_time": "2021-07-08T07:44:58.336Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(f'Arrival RMSE : {np.sqrt(mean_squared_error(y_a, predict_a)):.4f}')\n",
    "print(f'Arrival Training R^2 : {r2_score(y_a, predict_a) * 100:.4f}')\n",
    "print(f'Departure RMSE : {np.sqrt(mean_squared_error(y_d, predict_d)):.4f}')\n",
    "print(f'Departure Training R^2 : {r2_score(y_d, predict_d) * 100:.4f}')\n",
    "\n",
    "np.savetxt('p_val_a.csv', p_val_a, fmt='%.6f', delimiter=',')\n",
    "np.savetxt('p_val_d.csv', p_val_d, fmt='%.6f', delimiter=',')\n",
    "np.savetxt('predict_a.csv', predict_a, fmt='%.6f', delimiter=',')\n",
    "np.savetxt('predict_d.csv', predict_d, fmt='%.6f', delimiter=',')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b2b98",
   "metadata": {},
   "source": [
    "이렇게 각각의 model의(e.g. lightGBM, XGBOOST, Randomforest ...) Cross Validation을 예측한 결과와, Test data에 대해 예측한 결과를 파일로 저장 <br>\n",
    "-> 다음 stage에서는 CV결과들을 input data로 사용, 기존의 label은 그대로 사용, test data의 예측값을 다음 stage에서 test data의 input으로 사용 <br>\n",
    "-> 계속 쌓아갈 수 있음 (=Stacking, 보통 stage 1,2정도면 이후 효과는 미비)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b836ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T11:49:38.039928Z",
     "start_time": "2021-07-05T11:49:38.022938Z"
    }
   },
   "source": [
    "stacking, ensemble 보다 feature engineering, hyperparameter tuning의 성능 향상이 훨씬 커서 앞의 방법은 굳이 사용하지 않아도 괜찮음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2fe60c",
   "metadata": {},
   "source": [
    "## 왔다갔다 - 미완"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7366424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T14:44:48.194215Z",
     "start_time": "2021-07-08T14:44:48.173153Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "########################################################################################################################\n",
    "\n",
    "# 1. Prediction w/o arr_pred, dep_pred\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "y_a = Data_raw.AAR.to_numpy()\n",
    "X_a = Data_m.to_numpy()\n",
    "y_d = Data_raw.ADR.to_numpy()\n",
    "X_d = Data_m.to_numpy()\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size = 0.1, random_state = seed)\n",
    "#X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_train_a, y_train_a, test_size=0.11, random_state = 13) \n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size = 0.1, random_state = seed)\n",
    "#X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X_train_d, y_train_d, test_size=0.11, random_state = 13) \n",
    "\n",
    "\n",
    "# Model fitting\n",
    "temp_reg_arrival = LGBMRegressor(**params_a)\n",
    "temp_reg_arrival.fit(X_train_a, y_train_a,\n",
    "                     eval_set = [(X_test_a, y_test_a)],        # pairs to use as validation sets\n",
    "                     eval_metric = 'l2',                     \n",
    "                     early_stopping_rounds = 10 )            # loss fuction이 n번 이상 좋아지지 않으면 멈춰라\n",
    "\n",
    "temp_reg_departure = LGBMRegressor(**params_d)\n",
    "temp_reg_departure.fit(X_train_d, y_train_d,\n",
    "                       eval_set=[(X_test_d, y_test_d)],\n",
    "                       eval_metric='l2',\n",
    "                       early_stopping_rounds = 10)   \n",
    "\n",
    "\n",
    "# prediction\n",
    "arr_pred = temp_reg_arrival.predict(Data_m)\n",
    "dep_pred = temp_reg_departure.predict(Data_m)\n",
    "temp_prediction = pd.DataFrame({'arr_pred':arr_pred, 'dep_pred':dep_pred})\n",
    "\n",
    "\n",
    "# make Data_m - w/ arr_pred, dep_pred\n",
    "Data_train = Data_m.join(temp_prediction)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c07434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T14:52:16.131468Z",
     "start_time": "2021-07-08T14:52:16.082340Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "########################################################################################################################\n",
    "\n",
    "# 2. Prediction w/ arr_pred, dep_pred no.1\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "y_a = Data_raw.AAR.to_numpy()\n",
    "X_a = Data_train.drop('arr_pred', axis=1).to_numpy()\n",
    "y_d = Data_raw.ADR.to_numpy()\n",
    "X_d = Data_train.drop('dep_pred', axis=1).to_numpy()\n",
    "\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size = 0.1, random_state = seed)\n",
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_train_a, y_train_a, test_size=0.11, random_state = 13) \n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size = 0.1, random_state = seed)\n",
    "X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X_train_d, y_train_d, test_size=0.11, random_state = 13) \n",
    "\n",
    "# Model fitting\n",
    "reg_arrival = LGBMRegressor(**params_a)\n",
    "reg_arrival.fit(X_train_a, y_train_a,\n",
    "                eval_set = [(X_test_a, y_test_a)],        # pairs to use as validation sets\n",
    "                eval_metric = 'l2',                     \n",
    "                early_stopping_rounds = 10 )            # loss fuction이 n번 이상 좋아지지 않으면 멈춰라\n",
    "\n",
    "reg_departure = LGBMRegressor(**params_d)\n",
    "reg_departure.fit(X_train_d, y_train_d,\n",
    "                  eval_set=[(X_test_d, y_test_d)],\n",
    "                  eval_metric='l2',\n",
    "                  early_stopping_rounds = 10)   \n",
    "\n",
    "# prediction\n",
    "arr_pred = reg_arrival.predict(Data_train.drop('arr_pred', axis=1))\n",
    "dep_pred = reg_departure.predict(Data_train.drop('dep_pred', axis=1))\n",
    "temp_prediction = pd.DataFrame({'arr_pred':arr_pred, 'dep_pred':dep_pred})\n",
    "\n",
    "\n",
    "# make Data_m - w/ arr_pred, dep_pred\n",
    "Data_train = Data_m.join(temp_prediction)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f67d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T14:52:16.131468Z",
     "start_time": "2021-07-08T14:52:16.082340Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "########################################################################################################################\n",
    "\n",
    "# 3. Prediction w/ arr_pred, dep_pred no.2\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Data\n",
    "y_a = Data_raw.AAR.to_numpy()\n",
    "X_a = Data_train.drop('arr_pred', axis=1).to_numpy()\n",
    "y_d = Data_raw.ADR.to_numpy()\n",
    "X_d = Data_train.drop('dep_pred', axis=1).to_numpy()\n",
    "\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size = 0.1, random_state = seed)\n",
    "X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_train_a, y_train_a, test_size=0.11, random_state = 13) \n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d, y_d, test_size = 0.1, random_state = seed)\n",
    "X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X_train_d, y_train_d, test_size=0.11, random_state = 13) \n",
    "\n",
    "# Model fitting\n",
    "reg_arrival = LGBMRegressor(**params_a)\n",
    "reg_arrival.fit(X_train_a, y_train_a,\n",
    "                eval_set = [(X_test_a, y_test_a)],        # pairs to use as validation sets\n",
    "                eval_metric = 'l2',                     \n",
    "                early_stopping_rounds = 10 )            # loss fuction이 n번 이상 좋아지지 않으면 멈춰라\n",
    "\n",
    "reg_departure = LGBMRegressor(**params_d)\n",
    "reg_departure.fit(X_train_d, y_train_d,\n",
    "                  eval_set=[(X_test_d, y_test_d)],\n",
    "                  eval_metric='l2',\n",
    "                  early_stopping_rounds = 10)   \n",
    "\n",
    "# prediction\n",
    "arr_pred = reg_arrival.predict(Data_train.drop('arr_pred', axis=1))\n",
    "dep_pred = reg_departure.predict(Data_train.drop('dep_pred', axis=1))\n",
    "temp_prediction = pd.DataFrame({'arr_pred':arr_pred, 'dep_pred':dep_pred})\n",
    "\n",
    "\n",
    "# make Data_m - w/ arr_pred, dep_pred\n",
    "Data_train = Data_m.join(temp_prediction)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "\n",
    "# 아래 모든 Data_m을 Data_train으로 전부 바꿔야 함\n",
    "\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a9034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T15:07:51.228935Z",
     "start_time": "2021-07-08T15:07:51.220954Z"
    }
   },
   "source": [
    "AAR, ADR없는 Data_m을 temp_reg_arrival, temp_reg_departure로 예측함\n",
    "\n",
    "temp_reg_arrival, temp_reg_departure로 예측한 것을 arr_pred, dep_pred로 저장한 뒤에 \n",
    "\n",
    "Data_m에 합친 것을 Data_train으로 저장함\n",
    "\n",
    "Data_train이라는 DataFrame으로 reg_arrival, reg_departure을 학습시킴\n",
    "\n",
    "->> 근데 reg_arrival, reg_departured의 예측값이 더 안 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb0825",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd992057",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e319a",
   "metadata": {},
   "source": [
    "<sklearn.metrics> <br>\n",
    "\n",
    "* regression <br>\n",
    "mean_squared_error, mean_absolute_error, r2_score <br><br>\n",
    "\n",
    "* classification <br>\n",
    "log_loss, roc_auc_score, accuracy_score, confusion_matrix <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2bfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:04:28.806712Z",
     "start_time": "2021-07-09T09:04:28.789235Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def Test(Data_raw, ops='arrival', start=0, end = 10):        # ops : arrival , departure\n",
    "    if ops == 'arrival':\n",
    "        Data_raw = Data_raw.drop('ADR', axis=1)\n",
    "        datelist = pd.DataFrame(pd.date_range('2019-01-01', '2019-12-31 23:00', freq = '1h'))\n",
    "        test_result = pd.DataFrame({'Date' : datelist[start:end][0],\n",
    "                                    'EAD' : Data_raw['EAD'][start:end],\n",
    "                                    'Actual AAR' : Data_raw['AAR'][start:end], \n",
    "                                    'Predicted AAR' : reg_arrival.predict(Data_raw.drop('AAR', axis=1)[start:end]), \n",
    "                                    'Difference' : reg_arrival.predict(Data_raw.drop('AAR', axis=1)[start:end]) - Data_raw['AAR'][start:end]})\n",
    "    elif ops == 'departure':\n",
    "        Data_raw = Data_raw.drop('AAR', axis=1)        \n",
    "        datelist = pd.DataFrame(pd.date_range('2019-01-01', '2019-12-31 23:00', freq = '1h'))\n",
    "        test_result = pd.DataFrame({'Date' : datelist[start:end][0],\n",
    "                                    'EDD' : Data_raw['EDD'][start:end],\n",
    "                                    'Actual ADR' : Data_raw['ADR'][start:end], \n",
    "                                    'Predicted ADR' : reg_departure.predict(Data_raw.drop('ADR', axis=1)[start:end]), \n",
    "                                    'Difference' : reg_departure.predict(Data_raw.drop('ADR', axis=1)[start:end]) - Data_raw['ADR'][start:end]})\n",
    "        \n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d00c3",
   "metadata": {},
   "source": [
    "***\n",
    "## Arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc199b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:04:39.463088Z",
     "start_time": "2021-07-09T09:04:35.167465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict & Evaluation\n",
    "\n",
    "y_hat_a = reg_arrival.predict(X_a)\n",
    "print(f'RMSE : {np.sqrt(mean_squared_error(y_a, y_hat_a)):.4f}')\n",
    "print(f'Training R^2 : {r2_score(y_train_a, reg_arrival.predict(X_train_a)) * 100:.4f}')\n",
    "print(f'Test R^2 : {r2_score(y_test_a, reg_arrival.predict(X_test_a)) * 100:.4f}')\n",
    "\n",
    "#print(f'explained variance score : {explained_variance_score(y_a, y_hat_a) :.4f}')\n",
    "#print(f'max error : {max_error(y_a, y_hat_a) :.4f}')\n",
    "#print(f'mean absolute error : {mean_absolute_error(y_a, y_hat_a) :.4f}')\n",
    "#print(f'mean absolute percentage error : {mean_absolute_percentage_error(y_a, y_hat_a) :.4f}')\n",
    "#print(f'mean squared log error : {mean_squared_log_error(y_a, y_hat_a) :.4f}')\n",
    "#print(f'median absolute error : {median_absolute_error(y_a, y_hat_a) :.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940488ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:04:41.402131Z",
     "start_time": "2021-07-09T09:04:40.850743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "imp = pd.DataFrame({'feature': Data_m.columns, 'importance': reg_arrival.feature_importances_})\n",
    "imp = imp.sort_values('importance').set_index('feature')\n",
    "imp.plot(kind='barh', figsize = (20,20))\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980fc7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:04:53.970296Z",
     "start_time": "2021-07-09T09:04:53.786746Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metric(reg_arrival, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfaa6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:04:59.322721Z",
     "start_time": "2021-07-09T09:04:55.920909Z"
    }
   },
   "outputs": [],
   "source": [
    "# AAR - pred\n",
    "pred_test_a = Data_raw.drop('ADR', axis=1)\n",
    "pred_test_a['pred'] = reg_arrival.predict(pred_test_a.drop('AAR', axis=1))\n",
    "sns.pairplot(data=pred_test_a, vars=['AAR','pred'], size=10, plot_kws={'alpha': .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3f8a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:05:05.031219Z",
     "start_time": "2021-07-09T09:05:00.183850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "Test_all = Test(Data_raw, ops='arrival', start = 0, end = 8760).sort_values('Difference')\n",
    "Test_all['Difference'].abs().sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd6528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T06:13:25.606464Z",
     "start_time": "2021-07-02T06:13:25.593482Z"
    }
   },
   "source": [
    "-------------------\n",
    "## Departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30080786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:05:30.006504Z",
     "start_time": "2021-07-09T09:05:24.664347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict & Evaluation\n",
    "print(f'RMSE : {np.sqrt(mean_squared_error(y_d, reg_departure.predict(X_d))):.4f}')\n",
    "print(f'Training R^2 : {r2_score(y_train_d, reg_departure.predict(X_train_d)) * 100:.4f}')\n",
    "print(f'Test R^2 : {r2_score(y_test_d, reg_departure.predict(X_test_d)) * 100:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c61338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:05:31.400647Z",
     "start_time": "2021-07-09T09:05:30.897295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "imp = pd.DataFrame({'feature': Data_m.columns, 'importance': reg_departure.feature_importances_})\n",
    "imp = imp.sort_values('importance').set_index('feature')\n",
    "imp.plot(kind='barh', figsize = (20,20))\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0c494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:05:32.383459Z",
     "start_time": "2021-07-09T09:05:32.211532Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_metric(reg_departure, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27e7b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:05:36.760322Z",
     "start_time": "2021-07-09T09:05:33.162691Z"
    }
   },
   "outputs": [],
   "source": [
    "# ADR - pred\n",
    "pred_test_d = Data_raw.drop('AAR', axis=1)\n",
    "pred_test_d['pred'] = reg_departure.predict(pred_test_d.drop('ADR', axis=1))\n",
    "sns.pairplot(data=pred_test_d, vars=['ADR','pred'], size=10, plot_kws={'alpha': .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ebe47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:05:43.675033Z",
     "start_time": "2021-07-09T09:05:37.539110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "Test_all = Test(Data_raw, ops='departure', start = 0, end = 8760).sort_values('Difference')\n",
    "Test_all['Difference'].abs().sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc6a8f",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213caa7",
   "metadata": {},
   "source": [
    "# Maximum Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76463f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:06:29.842234Z",
     "start_time": "2021-07-09T09:06:29.822137Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 예전 버전 : 원래 demand부터 시작 -> extra demand를 더함\n",
    "\n",
    "def max_capacity(Data_raw, Data_m, example):   \n",
    "    \n",
    "    # extra\n",
    "    extra_demand = 50\n",
    "    \n",
    "    # arrival\n",
    "    data_a = Data_m.to_numpy()[example:example+1]\n",
    "    XX_a = np.zeros((1,len(data_a.T)))\n",
    "    for i in range(extra_demand+1):\n",
    "        XX_a = np.append(XX_a, data_a, axis = 0)\n",
    "        XX_a[i,0] = XX_a[i,0] + i-1\n",
    "    XXX_a = XX_a[1:extra_demand+1]\n",
    "    XXXX_a = np.arange(XXX_a[0,0], XXX_a[extra_demand-1,0]+1, 1)\n",
    "    YYYY_a = reg_arrival.predict(XXX_a[0:extra_demand+1])\n",
    "    max_aar = float(max(YYYY_a))\n",
    "    actual_aar = int(Data_raw['AAR'][example:example+1])\n",
    "    ead = int(Data_raw['EAD'][example:example+1])\n",
    "    prediction_a = float(reg_arrival.predict(Data_m[example:example+1]))\n",
    "    \n",
    "    # departure\n",
    "    data_d = Data_m.to_numpy()[example:example+1]\n",
    "    XX_d = np.zeros((1,len(data_d.T)))\n",
    "    for i in range(extra_demand+1):\n",
    "        XX_d = np.append(XX_d, data_d, axis = 0)\n",
    "        XX_d[i,2] = XX_d[i,2] + i-1\n",
    "    XXX_d = XX_d[1:extra_demand+1]\n",
    "    XXXX_d = np.arange(XXX_d[0,2], XXX_d[extra_demand-1,2]+1, 1)\n",
    "    YYYY_d = reg_departure.predict(XXX_d[0:extra_demand+1])\n",
    "    max_adr = float(max(YYYY_d))\n",
    "    actual_adr = int(Data_raw['ADR'][example:example+1])\n",
    "    edd = int(Data_raw['EDD'][example:example+1])\n",
    "    prediction_d = float(reg_departure.predict(Data_m[example:example+1])) \n",
    "\n",
    "    # Total\n",
    "    result_ = pd.DataFrame({'EAD' : XXXX_a ,'Predicted AAR' : YYYY_a, 'EDD' : XXXX_d ,'Predicted ADR' : YYYY_d, \n",
    "                            'Demand' : XXXX_a + YYYY_a, 'Capacity' : YYYY_a + YYYY_d})\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.title('Maximum Capacity', fontsize=30)\n",
    "    plt.xlabel('Estimated Demands', fontsize=25)\n",
    "    plt.ylabel('Capacity', fontsize=25)\n",
    "    plt.plot(XXXX_a, YYYY_a, label = 'Arrival')\n",
    "    plt.plot(XXXX_d, YYYY_d, label = 'Departure')\n",
    "    plt.plot(XXXX_a[:-15]+XXXX_d[:-15], YYYY_a[:-15] + YYYY_d[:-15], label = 'Total')   # 안 예뻐서 뒤에 15개 자름\n",
    "    plt.legend(prop={'size': 20})\n",
    "    ax = plt.subplot()\n",
    "    plt.text(0.95, 0.04,    # 위치조정\n",
    "             f\"\"\"\n",
    "             * Predicted Max AAR: {max_aar:.3f}\\n\n",
    "               Predicted AAR : {prediction_a:.3f}\\n\n",
    "               Actual AAR : {actual_aar} \\n               \n",
    "               EAD : {ead} \\n\\n\n",
    "             \n",
    "             * Predicted Max ADR: {max_adr:.3f}\\n\n",
    "               Predicted ADR : {prediction_d:.3f}\\n\n",
    "               Actual ADR : {actual_adr} \\n               \n",
    "               EDD : {edd} \\n\\n\n",
    "             \n",
    "             * Predicted Max Capacity: {max_aar + max_adr:.3f}\\n\n",
    "               Predicted Rate : {prediction_a + prediction_d:.3f}\\n\n",
    "               Actual Rate : {actual_aar + actual_adr} \\n               \n",
    "               Demand : {ead + edd}'\n",
    "             \"\"\",\n",
    "             fontsize=15, style='italic', transform=ax.transAxes, bbox={'facecolor': 'grey', 'alpha': 0, 'pad': 5})\n",
    "    plt.show()    \n",
    "    return result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941c682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:06:58.653062Z",
     "start_time": "2021-07-09T09:06:58.615824Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# *** training data에는 AAR, ADR 둘다 없어야 함\n",
    "# 0부터 시작\n",
    "# max capacity 선 추가\n",
    "# 원래 demand에 대한 predict 점 추가\n",
    "\n",
    "def max_capacity(Data_raw, Data_m, example):    # 50까지 늘림\n",
    "    \n",
    "    # extra\n",
    "    demand = 80\n",
    "    \n",
    "\n",
    "    # arrival\n",
    "    data_a = Data_m.to_numpy()[example:example+1]\n",
    "    XX_a = np.zeros((1,len(data_a.T)))\n",
    "    \n",
    "    original_demand_a = int(data_a[0][0])\n",
    "    for i in range(0,demand+1):\n",
    "        XX_a = np.append(XX_a, data_a, axis = 0)\n",
    "        XX_a[i,0] = XX_a[i,0] + i - original_demand_a\n",
    "    \n",
    "    XX_a[0] = XX_a[1]   \n",
    "    XX_a[0,0] = 0   \n",
    "    XXX_a = XX_a[0:demand+1]\n",
    "    XXXX_a = np.arange(XXX_a[0,0], XXX_a[demand,0]+1, 1)\n",
    "    YYYY_a = reg_arrival.predict(XXX_a[0:demand+1])\n",
    "    max_aar = float(max(YYYY_a))\n",
    "    actual_aar = int(Data_raw.drop('ADR', axis=1)['AAR'][example:example+1])\n",
    "    ead = int(Data_raw.drop('ADR', axis=1)['EAD'][example:example+1])\n",
    "    prediction_a = float(reg_arrival.predict(Data_m[example:example+1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # departure\n",
    "    data_d = Data_m.to_numpy()[example:example+1]\n",
    "    XX_d = np.zeros((1,len(data_d.T)))\n",
    "    \n",
    "    original_demand_d = int(data_d[0][1])\n",
    "    for i in range(0,demand+1):\n",
    "        XX_d = np.append(XX_d, data_d, axis = 0)\n",
    "        XX_d[i,1] = XX_d[i,1] + i - original_demand_d\n",
    "        \n",
    "    XX_d[0] = XX_d[1]           \n",
    "    XX_d[0,1] = 0\n",
    "    XXX_d = XX_d[0:demand+1]\n",
    "    XXXX_d = np.arange(XXX_d[0,1], XXX_d[demand,1]+1, 1)\n",
    "    YYYY_d = reg_departure.predict(XXX_d[0:demand+1])\n",
    "    max_adr = float(max(YYYY_d))\n",
    "    actual_adr = int(Data_raw.drop('AAR', axis=1)['ADR'][example:example+1])\n",
    "    edd = int(Data_raw.drop('AAR', axis=1)['EDD'][example:example+1])\n",
    "    prediction_d = float(reg_departure.predict(Data_m[example:example+1])) \n",
    "\n",
    "    \n",
    "    \n",
    "    # capacity - arrival과 departure을 수요의 비율로 늘렸을 때\n",
    "    max_cap = int(max(Data_raw['AAR'] + Data_raw['ADR']))\n",
    "    max_capacity = np.zeros(demand+1)\n",
    "    for i in range(0,demand*2+1,2):\n",
    "        if round(i*data_a[0,0]/(data_a[0,0]+data_d[0,1])) >=75:\n",
    "            capa_arr = YYYY_a[75]\n",
    "        elif round(i*data_d[0,1]/(data_a[0,0]+data_d[0,1])) >=75:\n",
    "            capa_dep = YYYY_d[75]\n",
    "        else:\n",
    "            capa_arr = YYYY_a[round(i*data_a[0,0]/(data_a[0,0]+data_d[0,1]))]\n",
    "            capa_dep = YYYY_d[round(i*data_d[0,1]/(data_a[0,0]+data_d[0,1]))]\n",
    "        max_capacity[int(i/2)] = capa_arr + capa_dep\n",
    "    \n",
    "    \n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.title('Maximum Capacity', fontsize=30)\n",
    "    plt.xlabel('Estimated Demands', fontsize=25)\n",
    "    plt.ylabel('Capacity', fontsize=25)\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    plt.plot(XXXX_a, YYYY_a, label = 'Arrival')    # Arrival\n",
    "    plt.plot(XXXX_d, YYYY_d, label = 'Departure')    # Departure\n",
    "    plt.plot(XXXX_a[:-5] + XXXX_d[:-5], max_capacity[:-5], label = 'Total')    # Capacity - 안 예뻐서 뒤에 55개 자름\n",
    "    plt.plot(XXXX_a[:-5]+XXXX_d[:-5], [max_cap]*(demand-4), label = f'Maximum Data : {max_cap}')    # 데이터 상 max capacity\n",
    "    \n",
    "    plt.plot(data_a[0,0], prediction_a,'ob', markersize = 10)    # 원래 arrival 예측값\n",
    "    plt.plot(data_d[0,1], prediction_d,'or', markersize = 10)    # 원래 departure 예측값\n",
    "    plt.plot(data_a[0,0]+data_d[0,1], prediction_a + prediction_d,'yo', markersize = 15)    # 원래 capacity 예측값\n",
    "    plt.legend(prop={'size': 20}, loc = 'upper left')\n",
    "\n",
    "    plt.text(0.95, 0.04,    # 위치조정\n",
    "             f\"\"\"\n",
    "             * Predicted Max AAR: {max_aar:.3f}\\n\n",
    "               Predicted AAR : {prediction_a:.3f}\\n\n",
    "               Actual AAR : {actual_aar} \\n               \n",
    "               EAD : {ead} \\n\\n\n",
    "             \n",
    "             * Predicted Max ADR: {max_adr:.3f}\\n\n",
    "               Predicted ADR : {prediction_d:.3f}\\n\n",
    "               Actual ADR : {actual_adr} \\n               \n",
    "               EDD : {edd} \\n\\n\n",
    "             \n",
    "             * Predicted Max Capacity: {max_aar + max_adr:.3f}\\n\n",
    "               Predicted Rate : {prediction_a + prediction_d:.3f}\\n\n",
    "               Actual Rate : {actual_aar + actual_adr} \\n               \n",
    "               Demand : {ead + edd}'\n",
    "             \"\"\",\n",
    "             fontsize=15, style='italic', transform=ax.transAxes, bbox={'facecolor': 'grey', 'alpha': 0, 'pad': 5})\n",
    "    \n",
    "    plt.show()   \n",
    "    \n",
    "    return Data_raw[example:example+1]   #result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84eaa5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:06:59.834559Z",
     "start_time": "2021-07-09T09:06:59.441117Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 원하는 시간의 Max Capacity 그래프\n",
    "\n",
    "max_capacity(Data_raw, Data_m, 101)     # 숫자에 원하는 Data index(0-8760) 넣기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ed1ea",
   "metadata": {},
   "source": [
    "< Notable Samples> \n",
    "\n",
    "* pd.DataFrame(Data_raw).idxmax()             # 각 column별 max값의 index\n",
    "\n",
    "AAR 최소 : 2 <br>\n",
    "AAR 최대 : 1481 <br>\n",
    "EAD 최대 : 641 <br>\n",
    "ADR 최소 : 28 <br>\n",
    "ADR 최대 : 538 <br>\n",
    "EDD 최대 : 273 <br>\n",
    "Total Rate(AAR+ADR) 최대(77) : 5703 <br>\n",
    "Total Demand(EAD+EDD) 최대(70) : 4814 <br><br>\n",
    "\n",
    "Arrival_reaminder 최대 : 5992 <br>\n",
    "Departure_remainder 최대 : 8242 <br><br>\n",
    "\n",
    "WSPD 최대 : 5990 <br>\n",
    "W_GST 최대 : 5990 <br>\n",
    "시정 최소 : 1755 <br>\n",
    "Ceiling 최소 : 321 <br>\n",
    "RVR 최소 1448 <br><br>\n",
    "\n",
    "TAF WSPD 최대 : 5988 <br>\n",
    "TAF W_GST 최대 : 5988 <br>\n",
    "TAF 시정 최소 : 319 <br>\n",
    "TAF Ceiling 최소 : 10?? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data : 불러온 파일\n",
    "# Data_temp : 일단 필요 없어 보이는 column을 버린 것\n",
    "# data_taf : dictinoary 안에 Data_6 - Data_24 전부 넣은 것\n",
    "# Data_6, Data_12 .. : 해당 시간 전의 TAF만 있고, 나머지 시간의 TAF는 버린 것\n",
    "# Data_raw : Data_#과 동일\n",
    "# Data_m : Data_# 에서 AAR, ADR 뺀 것\n",
    "# Data_a : Data_m 로 바꿈\n",
    "# Data_d : Data_m 로 바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e38ac3",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedeb85",
   "metadata": {},
   "source": [
    "# 추가 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3800d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T13:21:03.884649Z",
     "start_time": "2021-07-07T13:21:03.869492Z"
    }
   },
   "source": [
    "##  왜?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce337c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:01:19.013196Z",
     "start_time": "2021-07-09T01:01:18.982521Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AAR수 별로 정렬\n",
    "Data['AAR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f30b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:01:38.006544Z",
     "start_time": "2021-07-09T01:01:35.935549Z"
    }
   },
   "outputs": [],
   "source": [
    "# 예측한 값들 보기\n",
    "pred = pd.DataFrame({'pred' :reg_arrival.predict(Data_m)})\n",
    "print('9. : ', pred[(pred['pred'] < 10) & (pred['pred'] >= 9)].shape)\n",
    "print('10. : ', pred[(pred['pred'] < 11) & (pred['pred'] >= 10)].shape)\n",
    "print('11. : ', pred[(pred['pred'] < 12) & (pred['pred'] >= 11)].shape)\n",
    "print('12. : ', pred[(pred['pred'] < 13) & (pred['pred'] >= 12)].shape,'*')\n",
    "print('13. : ', pred[(pred['pred'] < 14) & (pred['pred'] >= 13)].shape,'*')\n",
    "print('14. : ', pred[(pred['pred'] < 15) & (pred['pred'] >= 14)].shape,'*')\n",
    "print('15. : ', pred[(pred['pred'] < 16) & (pred['pred'] >= 15)].shape,'*')\n",
    "print('16. : ', pred[(pred['pred'] < 17) & (pred['pred'] >= 16)].shape)\n",
    "print('17. : ', pred[(pred['pred'] < 18) & (pred['pred'] >= 17)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd2a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:01:45.916776Z",
     "start_time": "2021-07-09T01:01:45.801776Z"
    }
   },
   "outputs": [],
   "source": [
    "# 12에서 16으로 예측한 날들 표시\n",
    "sssss = pred[(pred['pred'] < 16) & (pred['pred'] >= 12)].index\n",
    "Data.loc[sssss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb46646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:03:09.243515Z",
     "start_time": "2021-07-09T01:03:08.595552Z"
    }
   },
   "outputs": [],
   "source": [
    "ddddd = Data[(Data['AAR'] >= 12)&(Data['AAR']<15)].index\n",
    "\n",
    "DD = Data[(Data['AAR'] >= 12)&(Data['AAR']<=16)]\n",
    "DDDD = pred.loc[ddddd].join(DD).head(30)\n",
    "\n",
    "sns.pairplot(data=DDDD, vars=['AAR','pred'], size=6, plot_kws={'alpha': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68953b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:03:10.866786Z",
     "start_time": "2021-07-09T01:03:10.646119Z"
    }
   },
   "outputs": [],
   "source": [
    "DDDD.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b55e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:03:14.829060Z",
     "start_time": "2021-07-09T01:03:14.697062Z"
    }
   },
   "outputs": [],
   "source": [
    "fea = 'WD'\n",
    "\n",
    "sns.distplot(DDDD[fea])\n",
    "\n",
    "print(\"AAR Skewness: %f\" % Data[fea].skew())\n",
    "print(\"ADR Kurtosis: %f\" % Data[fea].kurt())\n",
    "\n",
    "# P_Airp Arrival_remainder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a06c1",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7710eae",
   "metadata": {},
   "source": [
    "# 모델 완성 후 할 일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151c4c0",
   "metadata": {},
   "source": [
    "* data.py / training.py / evaluation.py 등 이렇게 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbf51f",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69096e6e",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641ec29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-08T10:28:03.089056Z",
     "start_time": "2021-07-08T10:27:59.572912Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "lgbr_a = \"../evaluate/lgbr_a.pkl\"\n",
    "lgbr_d = \"../evaluate/lgbr_d.pkl\"\n",
    "joblib.dump(reg_arrival, lgbr_a)\n",
    "joblib.dump(reg_departure, lgbr_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "189px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "503px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
